# ======================================================================
# MLOps SageMaker Pipeline - GitLab CI/CD
# ======================================================================
# Complete CI/CD pipeline for ML model training, evaluation, and deployment
# with autoscaling, rollback, and retraining capabilities.
# ======================================================================

stages:
  - validate
  - train
  - evaluate
  - register
  - deploy-staging
  - deploy-production
  - rollback

# ----------------------------------------------------------------------
# Global Configuration
# ----------------------------------------------------------------------
variables:
  AWS_DEFAULT_REGION: ${AWS_REGION}
  PYTHON_VERSION: "3.9"
  MODEL_PACKAGE_GROUP: "mlops-models"
  STAGING_ENDPOINT: "mlops-model-staging"
  PRODUCTION_ENDPOINT: "mlops-model-production"

default:
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install -q awscli boto3 sagemaker pyyaml

# ----------------------------------------------------------------------
# Templates
# ----------------------------------------------------------------------
.aws_auth: &aws_auth
  before_script:
    - pip install -q awscli boto3 sagemaker pyyaml pytest
    - aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID}
    - aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY}
    - aws configure set region ${AWS_DEFAULT_REGION}

# ======================================================================
# STAGE: Validate
# ======================================================================
lint:
  stage: validate
  script:
    - pip install flake8 black yamllint
    - flake8 src/ scripts/ --max-line-length=120 --ignore=E501,W503
    - black --check src/ scripts/
    - yamllint config/ monitoring/
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

unit-tests:
  stage: validate
  script:
    - pip install -r requirements-dev.txt
    - pytest tests/ -v --cov=src --cov-report=xml --cov-report=term
  coverage: '/TOTAL.*\s+(\d+%)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

validate-configs:
  stage: validate
  <<: *aws_auth
  script:
    - python -c "import yaml; yaml.safe_load(open('config/training_config.yaml'))"
    - python -c "import yaml; yaml.safe_load(open('config/deployment_config.yaml'))"
    - python -c "import yaml; yaml.safe_load(open('config/autoscaling_config.yaml'))"
    - echo "All configurations validated successfully"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"

# ======================================================================
# STAGE: Train
# ======================================================================
train-model:
  stage: train
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python -c "
      from src.pipelines.training_pipeline import create_training_pipeline
      from sagemaker import get_execution_role
      
      pipeline = create_training_pipeline(
          pipeline_name='MLOpsPipeline-${CI_COMMIT_SHORT_SHA}',
          role='${SAGEMAKER_EXECUTION_ROLE}',
          s3_bucket='${S3_BUCKET}',
      )
      
      # Create and start pipeline
      pipeline.upsert(role_arn='${SAGEMAKER_EXECUTION_ROLE}')
      execution = pipeline.start()
      print(f'Pipeline execution ARN: {execution.arn}')
      
      # Wait for completion
      execution.wait()
      status = execution.describe()['PipelineExecutionStatus']
      print(f'Pipeline status: {status}')
      
      if status != 'Succeeded':
          raise Exception(f'Pipeline failed with status: {status}')
      "
  artifacts:
    paths:
      - training_output.json
    expire_in: 7 days
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
  timeout: 4h

# ======================================================================
# STAGE: Evaluate
# ======================================================================
evaluate-model:
  stage: evaluate
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python -c "
      import boto3
      import json
      
      sm = boto3.client('sagemaker')
      
      # Get latest model from registry
      response = sm.list_model_packages(
          ModelPackageGroupName='${MODEL_PACKAGE_GROUP}',
          SortBy='CreationTime',
          SortOrder='Descending',
          MaxResults=1
      )
      
      if not response['ModelPackageSummaryList']:
          print('No models found in registry')
          exit(1)
      
      model_arn = response['ModelPackageSummaryList'][0]['ModelPackageArn']
      detail = sm.describe_model_package(ModelPackageName=model_arn)
      
      print(f'Latest model: {model_arn}')
      print(f'Status: {detail[\"ModelPackageStatus\"]}')
      
      # Save for next stage
      with open('model_info.json', 'w') as f:
          json.dump({'model_arn': model_arn}, f)
      "
  artifacts:
    paths:
      - model_info.json
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
  needs:
    - train-model

# ======================================================================
# STAGE: Register
# ======================================================================
approve-model:
  stage: register
  <<: *aws_auth
  script:
    - |
      python -c "
      import boto3
      import json
      
      with open('model_info.json') as f:
          model_arn = json.load(f)['model_arn']
      
      sm = boto3.client('sagemaker')
      sm.update_model_package(
          ModelPackageArn=model_arn,
          ModelApprovalStatus='Approved'
      )
      print(f'Model approved: {model_arn}')
      "
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
      allow_failure: false
  needs:
    - evaluate-model

# ======================================================================
# STAGE: Deploy Staging
# ======================================================================
deploy-staging:
  stage: deploy-staging
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python scripts/deploy_endpoint.py \
        --model-package-arn $(cat model_info.json | python -c "import sys,json; print(json.load(sys.stdin)['model_arn'])") \
        --endpoint-name ${STAGING_ENDPOINT} \
        --environment staging \
        --config-path config/deployment_config.yaml
  environment:
    name: staging
    url: https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/sagemaker/home?region=${AWS_DEFAULT_REGION}#/endpoints/${STAGING_ENDPOINT}
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
  needs:
    - evaluate-model

staging-smoke-test:
  stage: deploy-staging
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python -c "
      import boto3
      import json
      
      runtime = boto3.client('sagemaker-runtime')
      
      # Test inference
      response = runtime.invoke_endpoint(
          EndpointName='${STAGING_ENDPOINT}',
          ContentType='application/json',
          Body=json.dumps({'instances': [[0.1] * 10]})
      )
      
      result = json.loads(response['Body'].read())
      print(f'Smoke test result: {result}')
      print('Staging deployment verified!')
      "
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
  needs:
    - deploy-staging

# ======================================================================
# STAGE: Deploy Production
# ======================================================================
deploy-production:
  stage: deploy-production
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python scripts/deploy_endpoint.py \
        --model-package-arn $(cat model_info.json | python -c "import sys,json; print(json.load(sys.stdin)['model_arn'])") \
        --endpoint-name ${PRODUCTION_ENDPOINT} \
        --environment production \
        --config-path config/deployment_config.yaml
    - |
      python scripts/configure_autoscaling.py \
        --endpoint-name ${PRODUCTION_ENDPOINT} \
        --config-path config/autoscaling_config.yaml
  environment:
    name: production
    url: https://${AWS_DEFAULT_REGION}.console.aws.amazon.com/sagemaker/home?region=${AWS_DEFAULT_REGION}#/endpoints/${PRODUCTION_ENDPOINT}
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
      allow_failure: false
  needs:
    - staging-smoke-test
    - approve-model

production-health-check:
  stage: deploy-production
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - python scripts/deploy_endpoint.py --endpoint-name ${PRODUCTION_ENDPOINT} --health-check-only
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
  needs:
    - deploy-production

# ======================================================================
# STAGE: Rollback
# ======================================================================
rollback-production:
  stage: rollback
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python scripts/rollback_endpoint.py \
        --endpoint-name ${PRODUCTION_ENDPOINT} \
        --version previous \
        --reason "GitLab CI/CD triggered rollback"
  environment:
    name: production
    action: stop
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
      allow_failure: true

rollback-staging:
  stage: rollback
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python scripts/rollback_endpoint.py \
        --endpoint-name ${STAGING_ENDPOINT} \
        --version previous \
        --reason "GitLab CI/CD triggered rollback"
  environment:
    name: staging
    action: stop
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: manual
      allow_failure: true
    - if: $CI_COMMIT_BRANCH == "develop"
      when: manual
      allow_failure: true

# ======================================================================
# Scheduled: Retraining
# ======================================================================
scheduled-retrain:
  stage: train
  <<: *aws_auth
  script:
    - pip install -r requirements.txt
    - |
      python -c "
      from src.pipelines.retraining_pipeline import RetrainingPipeline
      
      pipeline = RetrainingPipeline(pipeline_name='MLOpsPipeline')
      pipeline.trigger_retraining(trigger_type='scheduled')
      "
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"

drift-check:
  stage: validate
  <<: *aws_auth
  script:
    - pip install -r requirements.txt scipy
    - |
      python scripts/data_drift_check.py \
        --baseline s3://${S3_BUCKET}/data/baseline/baseline.csv \
        --current s3://${S3_BUCKET}/data/current/current.csv \
        --threshold 0.05 \
        --output drift_report.json
  artifacts:
    paths:
      - drift_report.json
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
  allow_failure: true
